{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Traininig of the Inclusive classifier\n",
    "\n",
    "**4.3 Inclusive classifier** This trains the Inclusive classifier, a combination of the Particle-sequence classifier with the High Level Features classifier.\n",
    "\n",
    "To run this notebook we used the following configuration:\n",
    "* *Software stack*: Spark 2.4.3, analytics-zoo 0.5.1\n",
    "* *Platform*: CentOS 7, Python 3.6\n",
    "* *Spark cluster*: Analytix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # pip install pyspark or use your favorite way to set Spark Home, here we use findspark\n",
    "# import findspark\n",
    "# findspark.init('/home/luca/Spark/spark-2.4.3-bin-hadoop2.7') #set path to SPARK_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Configure according to your environment\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# pyspark_python = \"<path to python>/bin/python\"\n",
    "# analytics_zoo_jar = \"<path>/analytics-zoo-bigdl_0.8.0-spark_2.4.3-0.5.1-jar-with-dependencies.jar\"\n",
    "# analytics_zoo_python_api = \"<path>/analytics-zoo-bigdl_0.8.0-spark_2.4.3-0.5.1-python-api.zip\"\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#         .appName(\"4.3-Training-InclusiveClassifier\") \\\n",
    "#         .master(\"yarn\") \\\n",
    "#         .config(\"spark.driver.memory\",\"8g\") \\\n",
    "#         .config(\"spark.executor.memory\",\"14g\") \\\n",
    "#         .config(\"spark.executor.cores\",\"6\") \\\n",
    "#         .config(\"spark.executor.instances\",\"70\") \\\n",
    "#         .config(\"spark.dynamicAllocation.enabled\",\"false\") \\\n",
    "#         .config(\"spark.shuffle.reduceLocality.enabled\",\"false\") \\\n",
    "#         .config(\"spark.shuffle.blockTransferService\",\"nio\") \\\n",
    "#         .config(\"spark.scheduler.minRegisteredResourcesRatio\",\"1.0\") \\\n",
    "#         .config(\"spark.speculation\",\"false\") \\\n",
    "#         .config(\"spark.eventLog.enabled\",\"false\") \\\n",
    "#         .config(\"spark.jars\",analytics_zoo_jar) \\\n",
    "#         .config(\"spark.submit.pyFiles\",analytics_zoo_python_api) \\\n",
    "#         .config(\"spark.pyspark.python\",pyspark_python) \\\n",
    "#         .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jiao-ubuntu.sc.intel.com:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0ecc1a54d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if Spark Session has been created correctly\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PATH = \"file:///data/cern/cern-small/\"\n",
    "\n",
    "trainDF = spark.read.format('parquet')\\\n",
    "        .load(PATH + 'trainUndersampled.parquet')\\\n",
    "        .select(['GRU_input', 'HLF_input', 'encoded_label'])\n",
    "        \n",
    "testDF = spark.read.format('parquet')\\\n",
    "        .load(PATH + 'testUndersampled.parquet')\\\n",
    "        .select(['GRU_input', 'HLF_input', 'encoded_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- GRU_input: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- HLF_input: vector (nullable = true)\n",
      " |-- encoded_label: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of events in the test dataset:', 2123)\n",
      "('Number of events in the training dataset:', 8611)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of events in the test dataset:\", testDF.count())\n",
    "\n",
    "print(\"Number of events in the training dataset:\", trainDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Trying to search from: /tmp/spark-53d8fbd7-166b-44ff-9fd4-308508c219ff/userFiles-24f8e0da-5e83-46fb-8184-1d2453596498/analytics-zoo-bigdl_0.9.0-spark_2.4.3-0.6.0-SNAPSHOT-python-api.zip/zoo, but can not find the jar for Analytics-Zoo\n"
     ]
    }
   ],
   "source": [
    "# Init analytics zoo\n",
    "from zoo.common.nncontext import *\n",
    "sc = init_nncontext(\"Inclusive Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createZooKerasSequential\n",
      "creating: createZooKerasMasking\n",
      "creating: createZooKerasGRU\n",
      "creating: createZooKerasDropout\n",
      "creating: createZooKerasSequential\n",
      "creating: createZooKerasDropout\n",
      "creating: createZooKerasMerge\n",
      "creating: createZooKerasSequential\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasDense\n"
     ]
    }
   ],
   "source": [
    "from zoo.pipeline.api.keras.optimizers import Adam\n",
    "from zoo.pipeline.api.keras.models import Sequential\n",
    "from zoo.pipeline.api.keras.layers.core import *\n",
    "from zoo.pipeline.api.keras.layers.recurrent import GRU\n",
    "from zoo.pipeline.api.keras.engine.topology import Merge\n",
    "\n",
    "## GRU branch\n",
    "gruBranch = Sequential() \\\n",
    "            .add(Masking(0.0, input_shape=(801, 19))) \\\n",
    "            .add(GRU(\n",
    "                output_dim=50,\n",
    "                activation='tanh'\n",
    "            )) \\\n",
    "            .add(Dropout(0.2)) \\\n",
    "\n",
    "## HLF branch\n",
    "hlfBranch = Sequential() \\\n",
    "            .add(Dropout(0.2, input_shape=(14,)))\n",
    "\n",
    "## Concatenate the branches\n",
    "branches = Merge(layers=[gruBranch, hlfBranch], mode='concat')\n",
    "\n",
    "## Create the model\n",
    "model = Sequential() \\\n",
    "        .add(branches) \\\n",
    "        .add(Dense(25, activation='relu')) \\\n",
    "        .add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create train and validation RDD\n",
    "\n",
    "We need to create an RDD of `Sample`, a tuple of the form (`features`, `label`). The two elements of this touple should be `numpy arrays`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from bigdl.util.common import Sample\n",
    "import numpy as np\n",
    "\n",
    "trainRDD = trainDF.rdd.map(lambda row: Sample.from_ndarray(\n",
    "    [np.array(row.GRU_input), np.array(row.HLF_input)],\n",
    "    np.array(row.encoded_label)\n",
    "))\n",
    "\n",
    "testRDD = testDF.rdd.map(lambda row: Sample.from_ndarray(\n",
    "    [np.array(row.GRU_input), np.array(row.HLF_input)],\n",
    "    np.array(row.encoded_label)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sample: features: [JTensor: storage: [ 4.4452043  -1.4459659   0.12834984 ...  0.         28.284271\n",
       "  -2.0649471 ], shape: [801  19], float, JTensor: storage: [0.01069611 0.03581727 0.6577833  0.00202169 0.07692308 0.\n",
       "  0.00600872 0.6999402  0.29771337 0.76114357 0.         0.\n",
       "  0.         0.        ], shape: [14], float], labels: [JTensor: storage: [1. 0. 0.], shape: [3], float]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look at one element of trainRDD\n",
    "trainRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see that `Sample.feature` is  now composed by the list of 801 particles with 19 features each (`shape=[801 19]`) plus the HLF (`shape=[14]`) and the encoded label (`shape=[3]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Optimizer setup and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set of hyperparameters\n",
    "numEpochs =8\n",
    "\n",
    "# The batch used by BDL must be a multiple of numExecutors * executorCores\n",
    "# Because data will be equally distibuted inside each executor\n",
    "\n",
    "workerBatch =64\n",
    "\n",
    "numExecutors = 1\n",
    "# numExecutors = int(spark.conf.get('spark.executor.instances'))\n",
    "\n",
    "executorCores = 4\n",
    "# executorCores = int(spark.conf.get('spark.executor.cores'))\n",
    "\n",
    "BDLbatch = workerBatch * numExecutors * executorCores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createAdam\n",
      "creating: createCategoricalCrossEntropy\n",
      "creating: createZooKerasCategoricalAccuracy\n"
     ]
    }
   ],
   "source": [
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.nn.criterion import CategoricalCrossEntropy\n",
    "\n",
    "# optim_method = Adam(learningrate=0.002, learningrate_decay=0.0002, epsilon=9e-8)\n",
    "optim_method = Adam()\n",
    "\n",
    "model.compile(optimizer=optim_method, loss=CategoricalCrossEntropy(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's define a directory to store logs (i.e. train and validation losses) and save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving logs to /data/cern/Zoologs/InclusiveClassifier\n"
     ]
    }
   ],
   "source": [
    "# name of our application\n",
    "appName = \"InclusiveClassifier\"\n",
    "\n",
    "# Change it! \n",
    "logDir = \"/data/cern/Zoologs\"\n",
    "\n",
    "# Check if there is already an application with the same name \n",
    "# and remove it, otherwise logs will be appended to that app\n",
    "import os\n",
    "try:\n",
    "    os.system('rm -rf '+logDir+'/'+appName)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Saving logs to {}\".format(logDir+'/'+appName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.set_tensorboard(logDir, appName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We are now ready to launch the training.\n",
    "\n",
    "Warning relevant for CERN SWAN service users: During the training it would be better to shutdown the Toggle Spark Monitoring Display because each iteration is seen as a Spark job, therefore the toggle will try to display everything causing problem to the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%time model.fit(x=trainRDD, batch_size=BDLbatch, nb_epoch=numEpochs, validation_data=testRDD, distributed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "trainSummary = TrainSummary(log_dir=logDir,app_name=appName)\n",
    "loss = np.array(trainSummary.read_scalar(\"Loss\"))\n",
    "valSummary = ValidationSummary(log_dir=logDir,app_name=appName)\n",
    "val_loss = np.array(valSummary.read_scalar(\"Loss\"))\n",
    "\n",
    "plt.plot(loss[:,0], loss[:,1], label=\"Training loss\")\n",
    "plt.plot(val_loss[:,0], val_loss[:,1], label=\"Validation loss\", color='crimson', alpha=0.8)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Inclusive classifier loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "modelDir = logDir + '/models'\n",
    "model.saveModel(\n",
    "            modelPath = modelDir + '/' + appName + '.bigdl',\n",
    "            weightPath = modelDir + '/' + appName + '.bin',\n",
    "            over_write = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It is possible to load the model in the following way:\n",
    "```Python\n",
    "model = Model.loadModel(modelPath=modelPath+'.bigdl', weightPath=modelPath+'.bin')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(testRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.asarray(pred.collect())\n",
    "y_true = np.asarray(testDF.select('encoded_label').rdd\\\n",
    "                    .map(lambda row: np.asarray(row.encoded_label)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(3):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr[0], tpr[0], lw=2, \n",
    "         label='Inclusive classifier (AUC) = %0.4f' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Background Contamination (FPR)')\n",
    "plt.ylabel('Signal Efficiency (TPR)')\n",
    "plt.title('$tt$ selector')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy of the Inclusive classifier: {:.4f}'.format(\n",
    "    accuracy_score(np.argmax(y_true, axis=1),np.argmax(y_pred, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "labels_name = ['qcd', 'tt', 'wjets']\n",
    "labels = [0,1,2]\n",
    "\n",
    "cm = confusion_matrix(np.argmax(y_true, axis=1), np.argmax(y_pred, axis=1), labels=labels)\n",
    "\n",
    "## Normalize CM\n",
    "cm = cm / cm.astype(np.float).sum(axis=1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax = sns.heatmap(cm, annot=True, fmt='g')\n",
    "ax.xaxis.set_ticklabels(labels_name)\n",
    "ax.yaxis.set_ticklabels(labels_name)\n",
    "plt.title('Confusion matrix - Inclusive classifier')\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.jars",
     "value": "/eos/project/s/swan/public/BigDL/bigdl-SPARK_2.3-0.7.0-jar-with-dependencies.jar"
    },
    {
     "name": "spark.scheduler.minRegisteredResourcesRatio",
     "value": "1.0"
    },
    {
     "name": "spark.shuffle.reduceLocality.enabled",
     "value": "false"
    },
    {
     "name": "spark.shuffle.blockTransferService",
     "value": "nio"
    },
    {
     "name": "spark.dynamicAllocation.enabled",
     "value": "false"
    },
    {
     "name": "spark.speculation",
     "value": "false"
    },
    {
     "name": "spark.executor.instances",
     "value": "5"
    },
    {
     "name": "spark.executor.cores",
     "value": "4"
    },
    {
     "name": "spark.executor.memory",
     "value": "14G"
    },
    {
     "name": "spark.driver.memory",
     "value": "8G"
    },
    {
     "name": "spark.yarn.dist.files",
     "value": "/eos/project/s/swan/public/BigDL/bigdl-0.7.0-python-api.zip"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
