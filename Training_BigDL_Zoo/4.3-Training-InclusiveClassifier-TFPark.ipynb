{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we will train the Inclusive classifier, a combination of the Particle-sequence classifier with the High Level Features.\n",
    "\n",
    "To run this notebook we used the following configuration:\n",
    "* *Software stack*: LCG 94 (it has spark 2.3.1)\n",
    "* *Platform*: centos7-gcc7\n",
    "* *Spark cluster*: Hadalytic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jiao-ubuntu.sc.intel.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f59ec0e54d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if Spark Session has been created correctly\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add the BDL zip file\n",
    "# sc.addPyFile(\"/eos/project/s/swan/public/BigDL/bigdl-0.7.0-python-api.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PATH = \"file:///data/cern/cern-small/\"\n",
    "\n",
    "trainDF = spark.read.format('parquet')\\\n",
    "        .load(PATH + 'trainUndersampled.parquet')\\\n",
    "        .select(['GRU_input', 'HLF_input', 'encoded_label'])\n",
    "        \n",
    "testDF = spark.read.format('parquet')\\\n",
    "        .load(PATH + 'testUndersampled.parquet')\\\n",
    "        .select(['GRU_input', 'HLF_input', 'encoded_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- GRU_input: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- HLF_input: vector (nullable = true)\n",
      " |-- encoded_label: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Trying to search from: /tmp/spark-ca1340be-0073-4725-91aa-70f0d0c294ae/userFiles-da24c88c-51d3-40ba-acb1-01a1012a4f47/analytics-zoo-bigdl_0.9.0-spark_2.4.3-0.6.0-SNAPSHOT-python-api.zip/zoo, but can not find the jar for Analytics-Zoo\n"
     ]
    }
   ],
   "source": [
    "# Init analytics zoo\n",
    "from zoo.common.nncontext import *\n",
    "sc = init_nncontext(\"inclusive classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Input, Model\n",
    "from tensorflow.keras.layers import Masking, Dense, Activation, GRU, Dropout, concatenate\n",
    "# from zoo.pipeline.api.keras.layers.torch import Select\n",
    "# from zoo.pipeline.api.keras.layers mport BatchNormalization\n",
    "# from zoo.pipeline.api.keras.layers import GRU\n",
    "# from zoo.pipeline.api.keras.engine.topology import Merge\n",
    "\n",
    "## GRU branch\n",
    "gru_input = Input(shape=(801,19), name='gru_input')\n",
    "masking = Masking(mask_value=0.)(gru_input)\n",
    "gru = GRU(units=50,activation='tanh')(masking)\n",
    "gruBranch = Dropout(0.2)(gru)\n",
    "    \n",
    "hlf_input = Input(shape=(14,), name='hlf_input')\n",
    "hlfBranch = Dropout(0.2)(hlf_input)\n",
    "\n",
    "concat = concatenate([gruBranch, hlfBranch])\n",
    "dense = Dense(25, activation='relu')(concat)\n",
    "output = Dense(3, activation='softmax')(dense)\n",
    "    \n",
    "model = Model(inputs=[gru_input, hlf_input], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from bigdl.util.common import Sample\n",
    "import numpy as np\n",
    "\n",
    "trainRDD = trainDF.rdd.map(lambda row: ([np.array(row.GRU_input), np.array(row.HLF_input)],\n",
    "    np.array(row.encoded_label))\n",
    ")\n",
    "\n",
    "testRDD = testDF.rdd.map(lambda row: ([np.array(row.GRU_input), np.array(row.HLF_input)],\n",
    "    np.array(row.encoded_label))\n",
    ")\n",
    "\n",
    "\n",
    "# trainRDD = trainDF.rdd.map(lambda row: Sample.from_ndarray(\n",
    "#     [np.array(row.GRU_input), np.array(row.HLF_input)],\n",
    "#     np.array(row.encoded_label)\n",
    "# ))\n",
    "\n",
    "# testRDD = testDF.rdd.map(lambda row: Sample.from_ndarray(\n",
    "#     [np.array(row.GRU_input), np.array(row.HLF_input)],\n",
    "#     np.array(row.encoded_label)\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# trainRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# testRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create train and valiation Data\n",
    "\n",
    "We need to create an RDD of a tuple of the form (`features`, `label`). The two elements of this touple should be `numpy arrays`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([array([[ 4.44520407e+00, -1.44596592e+00,  1.28349836e-01, ...,\n",
       "            0.00000000e+00, -3.53553391e-02,  1.03635986e-02],\n",
       "          [ 2.37064971e+00, -4.94182636e-01,  8.14647870e-01, ...,\n",
       "            0.00000000e+00, -3.53553391e-02,  1.03635986e-02],\n",
       "          [ 4.30957519e+00, -1.93207888e+00, -7.64211272e-02, ...,\n",
       "            0.00000000e+00, -3.53553391e-02,  1.03635986e-02],\n",
       "          ...,\n",
       "          [-7.84960061e-02,  8.25741024e-01, -1.05017108e+00, ...,\n",
       "            0.00000000e+00, -3.53553391e-02, -2.06494702e+00],\n",
       "          [ 5.87193729e-01,  1.85925335e+00, -3.87692967e+00, ...,\n",
       "            0.00000000e+00, -3.53553391e-02,  2.08567421e+00],\n",
       "          [ 4.38693714e+00,  1.14162769e+01, -1.91426184e+01, ...,\n",
       "            0.00000000e+00,  2.82842712e+01, -2.06494702e+00]]),\n",
       "   array([0.01069611, 0.03581727, 0.6577833 , 0.00202169, 0.07692308,\n",
       "          0.        , 0.00600872, 0.69994022, 0.29771337, 0.76114355,\n",
       "          0.        , 0.        , 0.        , 0.        ])],\n",
       "  array([1., 0., 0.]))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look at one element of trainRDD\n",
    "trainRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see that `features` is  now composed by the list of 801 particles with 19 features each (`shape=[801 19]`) plus the HLF (`shape=[14]`) and the encoded label (`shape=[3]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from zoo.pipeline.api.net import TFDataset\n",
    "from zoo.tfpark.model import KerasModel\n",
    "\n",
    "# create TFDataset for TF training\n",
    "dataset = TFDataset.from_rdd(trainRDD,\n",
    "                                 features=[(tf.float32, [801, 19]), (tf.float32, [14])],\n",
    "                                 labels=(tf.float32, [3]),\n",
    "                                 batch_size=256,\n",
    "                                 val_rdd=testRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Optimizer setup and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set of hyperparameters\n",
    "numEpochs = 8\n",
    "\n",
    "# The batch used by BDL must be a multiple of numExecutors * executorCores\n",
    "# Because data will be equally distibuted inside each executor\n",
    "\n",
    "workerBatch = 64\n",
    "# numExecutors = int(sc._conf.get('spark.executor.instances'))\n",
    "numExecutors = 1\n",
    "# executorCores = int(sc._conf.get('spark.executor.cores'))\n",
    "executorCores = 4\n",
    "\n",
    "BDLbatch = workerBatch * numExecutors * executorCores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Use Keras model training API to train\n",
    "\n",
    "from bigdl.optim.optimizer import *\n",
    "# from bigdl.nn.criterion import CategoricalCrossEntropy\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "keras_model = KerasModel(model)\n",
    "\n",
    "\n",
    "# model.compile(optimizer='adam', loss=CategoricalCrossEntropy(), metrics=[Loss(CategoricalCrossEntropy())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's define a directory to store logs (i.e. train and validation losses) and save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving logs to /data/cern/TFParklogs/InclusiveClassifier\n"
     ]
    }
   ],
   "source": [
    "# name of our application\n",
    "appName = \"InclusiveClassifier\"\n",
    "\n",
    "# Change it! \n",
    "logDir = \"/data/cern/TFParklogs\"\n",
    "\n",
    "# Check if there is already an application with the same name \n",
    "# and remove it, otherwise logs will be appended to that app\n",
    "import os\n",
    "try:\n",
    "    os.system('rm -rf '+logDir+'/'+appName)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Saving logs to {}\".format(logDir+'/'+appName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createTrainSummary\n",
      "creating: createValidationSummary\n"
     ]
    }
   ],
   "source": [
    "# Set tensorboard for model training and validation\n",
    "# keras_model.set_tensorboard(logDir, appName)\n",
    "trainSummary = TrainSummary(log_dir=logDir,app_name=appName)\n",
    "valSummary = ValidationSummary(log_dir=logDir,app_name=appName)\n",
    "keras_model.set_train_summary(trainSummary)\n",
    "keras_model.set_val_summary(valSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zoo.pipeline.api.net.tf_dataset.TFNdarrayDataset at 0x7f59b4917650>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /tmp/spark-ca1340be-0073-4725-91aa-70f0d0c294ae/userFiles-da24c88c-51d3-40ba-acb1-01a1012a4f47/analytics-zoo-bigdl_0.9.0-spark_2.4.3-0.6.0-SNAPSHOT-python-api.zip/zoo/util/tf.py:87: convert_variables_to_constants (from zoo.util.tf_graph_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /tmp/spark-ca1340be-0073-4725-91aa-70f0d0c294ae/userFiles-da24c88c-51d3-40ba-acb1-01a1012a4f47/analytics-zoo-bigdl_0.9.0-spark_2.4.3-0.6.0-SNAPSHOT-python-api.zip/zoo/util/tf_graph_util.py:283: extract_sub_graph (from zoo.util.tf_graph_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createAdam\n",
      "creating: createZooKerasCategoricalCrossEntropy\n",
      "creating: createLoss\n",
      "creating: createZooKerasCategoricalAccuracy\n",
      "creating: createTFTrainingHelper\n",
      "creating: createTFValidationMethod\n",
      "creating: createTFValidationMethod\n",
      "creating: createIdentityCriterion\n",
      "creating: createMaxEpoch\n",
      "creating: createDistriOptimizer\n",
      "creating: createEveryEpoch\n",
      "creating: createMaxEpoch\n"
     ]
    }
   ],
   "source": [
    "keras_model.fit(x=dataset, epochs=numEpochs, distributed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We are now ready to launch the training.\n",
    "\n",
    "**Warnign: During the trainign it would be better to shutdown the Toggle Spark Monitorin Display because each iteration is seen as a spark job, therefore the toggle will try to display everything causing problem to the browser.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "# model.fit(x=trainRDD, batch_size=BDLbatch, nb_epoch=numEpochs, validation_data=testRDD, distributed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "%matplotlib notebook\n",
    "\n",
    "# trainSummary = TrainSummary(log_dir=logDir,app_name=appName)\n",
    "loss = np.array(trainSummary.read_scalar(\"Loss\"))\n",
    "# valSummary = ValidationSummary(log_dir=logDir,app_name=appName)\n",
    "val_loss = np.array(valSummary.read_scalar(\"Loss\"))\n",
    "\n",
    "plt.plot(loss[:,0], loss[:,1], label=\"Training loss\")\n",
    "plt.plot(val_loss[:,0], val_loss[:,1], label=\"Validation loss\", color='crimson', alpha=0.8)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Particle sequence classifier loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "modelDir = os.path.join(logDir, \"models\", \"inclusive.model\")\n",
    "print modelDir\n",
    "keras_model.save_model(modelDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It is possible to load the model in the following way:\n",
    "```Python\n",
    "model = Model.loadModel(modelPath=modelPath+'.bigdl', weightPath=modelPath+'.bin')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "testRDD2 = testDF.rdd.map(lambda row: [np.array(row.GRU_input), np.array(row.HLF_input)])\n",
    "test_dataset = TFDataset.from_rdd(testRDD2,\n",
    "                                 features=[(tf.float32, [801, 19]), (tf.float32, [14])],\n",
    "                                 labels=None,\n",
    "                                 batch_per_thread=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predRDD = keras_model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result = predRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.squeeze(result)\n",
    "y_true = np.asarray(testDF.select('encoded_label').rdd\\\n",
    "                    .map(lambda row: np.asarray(row.encoded_label)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(3):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr[0], tpr[0], lw=2, \n",
    "         label='Inclusive classifier (AUC) = %0.4f' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Background Contamination (FPR)')\n",
    "plt.ylabel('Signal Efficiency (TPR)')\n",
    "plt.title('$tt$ selector')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = np.array(trainSummary.read_scalar(\"Accuracy\"))\n",
    "# valSummary = ValidationSummary(log_dir=logDir,app_name=appName)\n",
    "val_loss = np.array(valSummary.read_scalar(\"Accuracy\"))\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(history.history['acc'], label='train')\n",
    "plt.plot(history.history['val_acc'], label='validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title(\"HLF classifier accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.jars",
     "value": "/eos/project/s/swan/public/BigDL/bigdl-SPARK_2.3-0.7.0-jar-with-dependencies.jar"
    },
    {
     "name": "spark.scheduler.minRegisteredResourcesRatio",
     "value": "1.0"
    },
    {
     "name": "spark.shuffle.reduceLocality.enabled",
     "value": "false"
    },
    {
     "name": "spark.shuffle.blockTransferService",
     "value": "nio"
    },
    {
     "name": "spark.dynamicAllocation.enabled",
     "value": "false"
    },
    {
     "name": "spark.speculation",
     "value": "false"
    },
    {
     "name": "spark.executor.instances",
     "value": "5"
    },
    {
     "name": "spark.executor.cores",
     "value": "4"
    },
    {
     "name": "spark.executor.memory",
     "value": "14G"
    },
    {
     "name": "spark.driver.memory",
     "value": "8G"
    },
    {
     "name": "spark.yarn.dist.files",
     "value": "/eos/project/s/swan/public/BigDL/bigdl-0.7.0-python-api.zip"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
